"""
Step 09: Generate Summary Research Report
==========================================
Compiles all statistical results, tables, and figure references into a
structured text report suitable for inclusion in the research paper.

Input:  reports/tables/*.csv, reports/figures/*.png
Output: reports/research_summary.txt
        reports/key_findings.csv
"""

import sys, os
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
os.chdir(PROJECT_ROOT)

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")

TABLES = PROJECT_ROOT / "reports" / "tables"
FIGURES = PROJECT_ROOT / "reports" / "figures"
REPORT_PATH = PROJECT_ROOT / "reports" / "research_summary.txt"
KEY_FINDINGS_PATH = PROJECT_ROOT / "reports" / "key_findings.csv"

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def safe_read(filename, subdir=TABLES):
    path = subdir / filename
    if path.exists():
        return pd.read_csv(path)
    return None


def load_indexed():
    path = PROJECT_ROOT / "data" / "processed" / "indexed_data.csv"
    if path.exists():
        return pd.read_csv(path)
    return None


def section_header(title, char="="):
    line = char * 72
    return f"\n{line}\n{title}\n{line}\n"


# ---------------------------------------------------------------------------
# Report Sections
# ---------------------------------------------------------------------------

def section_overview(df, lines):
    lines.append(section_header("1. DATASET OVERVIEW"))
    if df is None:
        lines.append("  Dataset not available.\n")
        return

    n = len(df)
    n_cols = len(df.columns)
    lines.append(f"  Total companies: {n}")
    lines.append(f"  Total variables: {n_cols}")

    if "country" in df.columns:
        for c, cnt in df["country"].value_counts().items():
            lines.append(f"    {c}: {cnt} companies ({cnt/n*100:.1f}%)")

    if "sector" in df.columns:
        lines.append(f"  Sectors: {df['sector'].nunique()}")
        for s, cnt in df["sector"].value_counts().head(10).items():
            lines.append(f"    {s}: {cnt}")

    # Score summaries
    score_cols = ["ESG_composite", "financial_score", "market_score",
                  "operational_score", "pref_balanced"]
    avail = [c for c in score_cols if c in df.columns]
    if avail:
        lines.append("\n  Key Score Statistics:")
        lines.append(f"  {'Score':<25s} {'Mean':>8s} {'Std':>8s} {'Min':>8s} {'Max':>8s}")
        lines.append("  " + "-" * 57)
        for col in avail:
            vals = df[col].dropna()
            lines.append(
                f"  {col:<25s} {vals.mean():8.2f} {vals.std():8.2f} "
                f"{vals.min():8.2f} {vals.max():8.2f}"
            )
    lines.append("")


def section_normality(lines):
    lines.append(section_header("2. NORMALITY TESTS"))
    norm = safe_read("normality_tests.csv")
    if norm is None:
        lines.append("  Results not available.\n")
        return

    n_normal_sw = norm["normal_shapiro"].sum()
    n_normal_jb = norm["normal_jb"].sum()
    n_total = len(norm)
    lines.append(f"  Variables tested: {n_total}")
    lines.append(f"  Normal (Shapiro-Wilk, p>0.05): {n_normal_sw}/{n_total}")
    lines.append(f"  Normal (Jarque-Bera, p>0.05):  {n_normal_jb}/{n_total}")
    lines.append(f"  Normal (K-S, p>0.05):          {norm['normal_ks'].sum()}/{n_total}")
    lines.append("\n  Implication: Most scores are non-normally distributed, "
                 "supporting use of non-parametric tests.")
    lines.append("")


def section_correlations(lines):
    lines.append(section_header("3. CORRELATION ANALYSIS"))
    sig = safe_read("correlation_significance.csv")
    if sig is None:
        lines.append("  Results not available.\n")
        return

    n_sig_p = sig["sig_pearson"].sum()
    n_sig_s = sig["sig_spearman"].sum()
    n_total = len(sig)
    lines.append(f"  Total pairwise tests: {n_total}")
    lines.append(f"  Significant Pearson (p<0.05):  {n_sig_p} ({n_sig_p/max(1,n_total)*100:.1f}%)")
    lines.append(f"  Significant Spearman (p<0.05): {n_sig_s} ({n_sig_s/max(1,n_total)*100:.1f}%)")

    # Strongest correlations
    top_pos = sig.nlargest(5, "pearson_r")
    if len(top_pos) > 0:
        lines.append("\n  Strongest positive correlations:")
        for _, r in top_pos.iterrows():
            lines.append(f"    {r['var1']} <-> {r['var2']}: r={r['pearson_r']:.3f} (p={r['pearson_p']:.4f})")

    top_neg = sig.nsmallest(3, "pearson_r")
    if len(top_neg) > 0:
        lines.append("\n  Strongest negative correlations:")
        for _, r in top_neg.iterrows():
            lines.append(f"    {r['var1']} <-> {r['var2']}: r={r['pearson_r']:.3f} (p={r['pearson_p']:.4f})")
    lines.append("")


def section_esg_financial(lines):
    lines.append(section_header("4. ESG-FINANCIAL RELATIONSHIP"))
    reg = safe_read("esg_financial_regression.csv")
    if reg is None:
        lines.append("  Results not available.\n")
        return

    lines.append(f"  Regressions run: {len(reg)}")
    sig_results = reg[reg["p_value"] < 0.05]
    lines.append(f"  Significant (p<0.05): {len(sig_results)}/{len(reg)}")

    for _, r in reg.iterrows():
        sig_mark = "*" if r["p_value"] < 0.05 else " "
        lines.append(
            f"    {r['independent_var']} -> {r['dependent_var']}: "
            f"R2={r['r_squared']:.4f}, slope={r['slope']:.4f}, p={r['p_value']:.4f} {sig_mark}"
        )

    # Multiple regression
    mreg = safe_read("multiple_regression.csv")
    if mreg is not None and len(mreg) > 0:
        lines.append("\n  Multiple Regression (with controls):")
        for _, r in mreg.iterrows():
            sig_mark = "*" if r["p_value"] < 0.05 else " "
            lines.append(
                f"    {r['independent']} -> {r['dependent']}: "
                f"coef={r['coefficient']:.4f}, p={r['p_value']:.4f}, "
                f"R2_adj={r['adj_r_squared']:.4f} {sig_mark}"
            )
    lines.append("")


def section_sector_country(lines):
    lines.append(section_header("5. SECTOR AND COUNTRY DIFFERENCES"))

    # Sector ANOVA
    anova = safe_read("sector_anova.csv")
    if anova is not None:
        sig_anova = anova[anova["significant_anova"]]
        lines.append(f"  ANOVA tests: {len(anova)}")
        lines.append(f"  Significant (p<0.05): {len(sig_anova)}/{len(anova)}")
        for _, r in anova.iterrows():
            sig_mark = "*" if r["significant_anova"] else " "
            lines.append(
                f"    {r['variable']}: F={r['anova_F']:.2f}, p={r['anova_p']:.4f}, "
                f"eta2={r['eta_squared']:.3f} ({r['effect_size']}) {sig_mark}"
            )

    # Country
    country = safe_read("country_differences.csv")
    if country is not None:
        lines.append(f"\n  Country difference tests: {len(country)}")
        sig_c = country[country["significant_t"]]
        lines.append(f"  Significant (t-test, p<0.05): {len(sig_c)}/{len(country)}")
        for _, r in country.iterrows():
            sig_mark = "*" if r["significant_t"] else " "
            lines.append(
                f"    {r['variable']}: Cohen's d={r['cohens_d']:.3f} ({r['effect_size']}), "
                f"p={r['ttest_p']:.4f} {sig_mark}"
            )
    lines.append("")


def section_quintile(lines):
    lines.append(section_header("6. QUINTILE ANALYSIS (Q5 vs Q1)"))
    qtest = safe_read("quintile_q5_vs_q1.csv")
    if qtest is None:
        lines.append("  Results not available.\n")
        return

    lines.append(f"  Variables tested: {len(qtest)}")
    sig = qtest[qtest["significant"]]
    lines.append(f"  Significant differences: {len(sig)}/{len(qtest)}")
    for _, r in qtest.iterrows():
        sig_mark = "*" if r["significant"] else " "
        lines.append(
            f"    {r['variable']}: Q5={r['Q5_mean']:.3f}, Q1={r['Q1_mean']:.3f}, "
            f"diff={r['difference']:.3f}, d={r['cohens_d']:.3f} {sig_mark}"
        )
    lines.append("")


def section_factor_contribution(lines):
    lines.append(section_header("7. FACTOR CONTRIBUTIONS"))
    fc = safe_read("factor_contributions.csv")
    if fc is None:
        lines.append("  Results not available.\n")
        return

    lines.append("  Factor contributions to balanced preference score:")
    lines.append(f"  {'Factor':<30s} {'R2':>8s} {'Slope':>8s} {'p-value':>10s}")
    lines.append("  " + "-" * 56)
    for _, r in fc.iterrows():
        sig_mark = "*" if r["p_value"] < 0.05 else " "
        lines.append(
            f"  {r['factor']:<30s} {r['r_squared']:8.4f} {r['slope']:8.4f} "
            f"{r['p_value']:10.4f} {sig_mark}"
        )
    lines.append("")


def section_multicollinearity(lines):
    lines.append(section_header("8. MULTICOLLINEARITY (VIF)"))
    vif = safe_read("vif_multicollinearity.csv")
    if vif is None:
        lines.append("  Results not available.\n")
        return

    lines.append(f"  {'Factor':<30s} {'VIF':>8s} {'Level':>12s}")
    lines.append("  " + "-" * 50)
    for _, r in vif.iterrows():
        lines.append(f"  {r['factor']:<30s} {r['VIF']:8.2f} {r['multicollinearity']:>12s}")
    high_vif = vif[vif["VIF"] > 5]
    if len(high_vif) > 0:
        lines.append(f"\n  WARNING: {len(high_vif)} factor(s) with VIF > 5")
    else:
        lines.append("\n  All VIF values below 5 - no serious multicollinearity detected.")
    lines.append("")


def section_weight_sensitivity(lines):
    lines.append(section_header("9. WEIGHT SENSITIVITY ANALYSIS"))

    # Rank stability
    stab = safe_read("rank_stability.csv")
    if stab is not None:
        stable_pct = stab["rank_stable"].mean() * 100
        avg_std = stab["rank_std"].mean()
        avg_range = stab["rank_range"].mean()
        lines.append(f"  Rank stability (100 weight perturbations, +/-20%):")
        lines.append(f"    Stable companies (std < 3): {stable_pct:.1f}%")
        lines.append(f"    Average rank std:  {avg_std:.2f}")
        lines.append(f"    Average rank range: {avg_range:.1f}")

    # Grid search best
    grid = safe_read("weight_grid_search.csv")
    if grid is not None:
        best = grid.iloc[0]
        lines.append(f"\n  Grid search (ESG x Financial weights):")
        lines.append(f"    Best ESG weight: {best.get('esg_weight', 'N/A')}")
        lines.append(f"    Best Financial weight: {best.get('financial_weight', 'N/A')}")
        sharpe_val = best.get("score_sharpe", best.get("sharpe_like", best.get("return_sharpe", None)))
        if sharpe_val is not None and isinstance(sharpe_val, (int, float)):
            lines.append(f"    Best Sharpe: {sharpe_val:.4f}")
        else:
            lines.append(f"    Best Sharpe: N/A")
        lines.append(f"    Total combinations tested: {len(grid)}")

    # Profile comparison
    prof = safe_read("weight_profile_comparison.csv")
    if prof is not None:
        lines.append(f"\n  Profile comparison:")
        for _, r in prof.iterrows():
            lines.append(
                f"    {r['profile1']} vs {r['profile2']}: "
                f"Kendall tau={r['kendall_tau']:.3f}, "
                f"Top-10 overlap={r['top10_overlap']:.0f}"
            )
    lines.append("")


def section_benchmark(lines):
    lines.append(section_header("10. BENCHMARK COMPARISON"))
    summary = safe_read("benchmark_summary.csv")
    if summary is None:
        lines.append("  Results not available.\n")
        return

    for _, r in summary.iterrows():
        lines.append(f"  {r['index']}:")
        lines.append(f"    N companies: {r['n_companies']:.0f}")
        if pd.notna(r.get("avg_ESG")):
            lines.append(f"    Avg ESG:       {r['avg_ESG']:.1f}")
        if pd.notna(r.get("avg_financial")):
            lines.append(f"    Avg Financial: {r['avg_financial']:.1f}")
        if pd.notna(r.get("n_sectors")):
            lines.append(f"    N sectors:     {r['n_sectors']:.0f}")

    # Portfolio performance
    perf = safe_read("benchmark_portfolio_performance.csv")
    if perf is not None:
        mom_cols = [c for c in perf.columns if "avg_price_momentum" in c]
        if mom_cols:
            lines.append(f"\n  Simulated portfolio returns ({mom_cols[0]}):")
            for _, r in perf.iterrows():
                lines.append(f"    {r['portfolio']}: {r[mom_cols[0]]:.2f}%")
    lines.append("")


def section_advanced(lines):
    lines.append(section_header("11. ADVANCED ANALYSIS"))

    # PCA
    pca_var = safe_read("advanced_pca_variance.csv")
    if pca_var is not None:
        lines.append("  PCA - Explained Variance:")
        for _, r in pca_var.iterrows():
            lines.append(
                f"    {r['component']}: eigenvalue={r['eigenvalue']:.3f}, "
                f"var={r['variance_explained']:.1%}, cum={r['cumulative_variance']:.1%}"
            )
        n_retain = (pca_var["eigenvalue"] > 1).sum()
        lines.append(f"    Components to retain (Kaiser): {n_retain}")

    # Clustering
    clust = safe_read("advanced_cluster_metrics.csv")
    if clust is not None:
        best = clust.loc[clust["silhouette_score"].idxmax()]
        lines.append(f"\n  Hierarchical Clustering:")
        lines.append(f"    Optimal clusters: {best['n_clusters']:.0f}")
        lines.append(f"    Silhouette score: {best['silhouette_score']:.3f}")

    # Bootstrap
    boot = safe_read("advanced_bootstrap_ci.csv")
    if boot is not None:
        stable_pct = boot["rank_stable"].mean() * 100
        avg_ci = boot["ci_width"].mean()
        lines.append(f"\n  Bootstrap CI (500 iterations):")
        lines.append(f"    Stable companies: {stable_pct:.1f}%")
        lines.append(f"    Average CI width: {avg_ci:.1f} positions")

    # Factor ablation
    abl = safe_read("advanced_factor_ablation.csv")
    if abl is not None:
        lines.append(f"\n  Factor Ablation:")
        for _, r in abl.iterrows():
            lines.append(
                f"    Remove {r['removed_factor']}: Kendall tau={r['kendall_tau']:.3f}, "
                f"mean rank shift={r['mean_rank_shift']:.1f}"
            )

    # Efficient frontier
    opt = safe_read("advanced_optimal_portfolios.csv")
    if opt is not None:
        lines.append(f"\n  Efficient Frontier:")
        for _, r in opt.iterrows():
            lines.append(
                f"    {r['portfolio']}: return={r['return']:.2f}, "
                f"risk={r['risk']:.2f}, sharpe={r['sharpe']:.3f}"
            )

    # Cross-validation
    cv = safe_read("advanced_cv_weights.csv")
    if cv is not None:
        lines.append(f"\n  Cross-Validation ({len(cv)}-fold):")
        lines.append(f"    Avg train Sharpe: {cv['train_sharpe'].mean():.3f}")
        lines.append(f"    Avg test Sharpe:  {cv['test_sharpe'].mean():.3f}")
        lines.append(f"    Overfit ratio:    {cv['overfit_ratio'].mean():.2f}")

    # Gini
    gini = safe_read("advanced_gini_inequality.csv")
    if gini is not None:
        lines.append(f"\n  Score Inequality (Gini):")
        for _, r in gini.iterrows():
            lines.append(
                f"    {r['score']}: Gini={r['gini_coefficient']:.3f} ({r['inequality']})"
            )
    lines.append("")


def section_multi_horizon(lines):
    lines.append(section_header("12. MULTI-HORIZON RETURN COMPARISON"))
    mh = safe_read("benchmark_multi_horizon.csv")
    if mh is None:
        lines.append("  Results not available.\n")
        return

    horizons = [c.replace("return_", "") for c in mh.columns if c.startswith("return_")]
    lines.append(f"  {'Strategy':<28s}" + "".join(f"{'ret_'+h:>10s}{'shrp_'+h:>8s}" for h in horizons))
    lines.append("  " + "-" * (28 + len(horizons) * 18))
    for _, r in mh.iterrows():
        line = f"  {r['strategy']:<28s}"
        for h in horizons:
            ret_val = r.get(f"return_{h}", 0)
            sh_val = r.get(f"sharpe_{h}", 0)
            line += f"{ret_val:+10.2f}{sh_val:+8.3f}"
        lines.append(line)

    # Highlight: does multi-factor beat universe across horizons?
    mf_row = mh[mh["strategy"].str.contains("MultiF|balanced", case=False, na=False)]
    univ_row = mh[mh["strategy"].str.contains("Universe", case=False, na=False)]
    if len(mf_row) > 0 and len(univ_row) > 0:
        beats = 0
        total = 0
        for h in horizons:
            mf_ret = mf_row.iloc[0].get(f"return_{h}", 0)
            u_ret = univ_row.iloc[0].get(f"return_{h}", 0)
            if mf_ret > u_ret:
                beats += 1
            total += 1
        lines.append(f"\n  Multi-factor beats universe: {beats}/{total} horizons")
    lines.append("")


def section_alpha_beta(lines):
    lines.append(section_header("13. ALPHA/BETA DECOMPOSITION"))
    ab = safe_read("benchmark_alpha_beta.csv")
    if ab is None:
        lines.append("  Results not available.\n")
        return

    lines.append(f"  {'Strategy':<28s} {'Alpha':>8s} {'Beta':>6s} {'Excess':>8s} {'IR':>8s}")
    lines.append("  " + "-" * 60)
    for _, r in ab.iterrows():
        lines.append(
            f"  {r['strategy']:<28s} {r['alpha']:+8.2f} {r['beta']:6.2f} "
            f"{r['excess_return']:+8.2f} {r['information_ratio']:+8.3f}"
        )
    lines.append("")


def section_regime(lines):
    lines.append(section_header("14. MARKET REGIME ANALYSIS"))
    reg = safe_read("advanced_regime_analysis.csv")
    if reg is None:
        lines.append("  Results not available.\n")
        return

    for regime in ["bull", "bear"]:
        sub = reg[reg["regime"] == regime]
        if len(sub) > 0:
            lines.append(f"\n  {regime.upper()} Market:")
            for _, r in sub.iterrows():
                lines.append(
                    f"    {r['strategy']:<20s}: return={r['avg_return']:+6.2f}%, "
                    f"excess={r['excess_return']:+6.2f}%, sharpe={r['sharpe']:+.3f}"
                )
    lines.append("")


def section_monotonicity(lines):
    lines.append(section_header("15. FACTOR MONOTONICITY"))
    mono = safe_read("advanced_factor_monotonicity.csv")
    if mono is None:
        lines.append("  Results not available.\n")
        return

    lines.append(f"  {'Factor':<25s} {'Q1':>6s} {'Q3':>6s} {'Q5':>6s} {'Spread':>8s} {'Mono':>6s}")
    lines.append("  " + "-" * 57)
    for _, r in mono.iterrows():
        lines.append(
            f"  {r['factor']:<25s} {r['Q1_return']:+6.1f} {r['Q3_return']:+6.1f} "
            f"{r['Q5_return']:+6.1f} {r['Q5_Q1_spread']:+8.2f} {r['monotonic']:>6s}"
        )

    # Key finding
    mono_yes = mono[mono["monotonic"] == "Yes"]
    lines.append(f"\n  Monotonic factors: {len(mono_yes)}/{len(mono)}")
    if len(mono_yes) > 0:
        best = mono_yes.nlargest(1, "Q5_Q1_spread").iloc[0]
        lines.append(f"  Best predictor: {best['factor']} (Q5-Q1 spread = {best['Q5_Q1_spread']:+.2f}%)")
    lines.append("")


def section_data_quality(lines):
    lines.append(section_header("16. DATA QUALITY & VARIABLE CLASSIFICATION"))

    # Variable classification
    vclass = safe_read("variable_type_classification.csv")
    if vclass is not None:
        type_counts = vclass["type"].value_counts()
        lines.append(f"  Variable type classification ({len(vclass)} variables):")
        for vtype, cnt in type_counts.items():
            lines.append(f"    {vtype:<20s}: {cnt}")
    else:
        lines.append("  Variable classification not available.")

    # Outlier report
    outlier = safe_read("outlier_report.csv")
    if outlier is not None:
        n_treated = (outlier["n_consensus_outliers"] > 0).sum()
        total_outliers = outlier["n_consensus_outliers"].sum()
        lines.append(f"\n  Advanced Outlier Detection (IQR + MAD + Z-score consensus):")
        lines.append(f"    Variables with outliers: {n_treated}/{len(outlier)}")
        lines.append(f"    Total outlier observations flagged: {total_outliers:.0f}")
        top_outlier = outlier.nlargest(5, "n_consensus_outliers")
        if len(top_outlier) > 0:
            lines.append("    Most affected variables:")
            for _, r in top_outlier.iterrows():
                lines.append(
                    f"      {r['variable']:<30s}: {r['n_consensus_outliers']:.0f} outliers "
                    f"({r['pct_consensus']:.1f}%)"
                )

    # Data quality before/after
    dq_before = safe_read("data_quality_before.csv")
    dq_after = safe_read("data_quality_after.csv")
    if dq_before is not None and dq_after is not None:
        avg_miss_before = dq_before["missing_pct"].mean() if "missing_pct" in dq_before.columns else 0
        avg_miss_after = dq_after["missing_pct"].mean() if "missing_pct" in dq_after.columns else 0
        lines.append(f"\n  Data Quality:")
        lines.append(f"    Avg missing before cleaning: {avg_miss_before:.1f}%")
        lines.append(f"    Avg missing after cleaning:  {avg_miss_after:.1f}%")
    lines.append("")


def section_binary_ordinal(lines):
    lines.append(section_header("17. BINARY & ORDINAL VARIABLE ANALYSIS"))

    # Binary variable analysis
    bva = safe_read("binary_variable_analysis.csv")
    if bva is not None:
        lines.append(f"  Binary variables tested: {len(bva)}")
        # Check for significant point-biserial correlations with key scores
        for score_name in ["ESG_composite", "financial_score", "pref_balanced"]:
            sig_col = f"pb_sig_{score_name}"
            r_col = f"pb_r_{score_name}"
            p_col = f"pb_p_{score_name}"
            if sig_col in bva.columns:
                sig_count = bva[sig_col].sum()
                lines.append(f"    Significant associations with {score_name}: {sig_count}/{len(bva)}")

        lines.append("")
        for _, r in bva.iterrows():
            esg_sig = "*" if r.get("pb_sig_ESG_composite", False) else " "
            fin_sig = "*" if r.get("pb_sig_financial_score", False) else " "
            pref_sig = "*" if r.get("pb_sig_pref_balanced", False) else " "
            lines.append(
                f"    {r['variable']:<30s}: prop={r['proportion_yes']:.2f}, "
                f"ESG r={r.get('pb_r_ESG_composite', 0):.3f}{esg_sig}, "
                f"Fin r={r.get('pb_r_financial_score', 0):.3f}{fin_sig}, "
                f"Pref r={r.get('pb_r_pref_balanced', 0):.3f}{pref_sig}"
            )
    else:
        lines.append("  Binary variable analysis not available.")

    # Phi coefficients
    phi = safe_read("binary_phi_coefficients.csv")
    if phi is not None and len(phi) > 0:
        lines.append(f"\n  Phi Coefficients (binary-binary associations):")
        for _, r in phi.iterrows():
            sig_mark = "*" if r.get("p_value", 1) < 0.05 else " "
            lines.append(
                f"    {r['var1']} x {r['var2']}: "
                f"phi={r['phi_coefficient']:.3f}, p={r['p_value']:.4f} {sig_mark}"
            )

    # Ordinal variable analysis
    ova = safe_read("ordinal_variable_analysis.csv")
    if ova is not None:
        lines.append(f"\n  Ordinal variables tested: {len(ova)}")
        for _, r in ova.iterrows():
            kw_p = r.get("kruskal_p_sector", 1.0)
            sig_mark = "*" if kw_p < 0.05 else " "
            rho_pref = r.get("spearman_r_pref_balanced", 0)
            rho_p = r.get("spearman_p_pref_balanced", 1.0)
            lines.append(
                f"    {r['variable']}: median={r['median']:.0f}, "
                f"KW-H(sector)={r.get('kruskal_H_sector', 0):.3f} (p={kw_p:.4f}){sig_mark}, "
                f"Spearman(pref)={rho_pref:.3f} (p={rho_p:.4f})"
            )
    else:
        lines.append("\n  Ordinal variable analysis not available.")
    lines.append("")


def section_nonparametric(lines):
    lines.append(section_header("18. NON-PARAMETRIC ROBUSTNESS TESTS"))

    # Friedman test
    friedman = safe_read("friedman_test.csv")
    if friedman is not None and len(friedman) > 0:
        r = friedman.iloc[0]
        lines.append(
            f"  Friedman test (across score categories):"
        )
        lines.append(
            f"    Chi-squared: {r.get('chi_squared', r.get('statistic', 0)):.3f}"
        )
        lines.append(
            f"    p-value:     {r.get('p_value', 0):.4f}"
        )
        if r.get("p_value", r.get("p_value", 1)) < 0.05:
            lines.append("    Result: Significant differences in score distributions across categories.")
        else:
            lines.append("    Result: No significant differences detected.")
    else:
        lines.append("  Friedman test not available.")

    # Wilcoxon pairwise
    wilcoxon = safe_read("wilcoxon_pairwise.csv")
    if wilcoxon is not None and len(wilcoxon) > 0:
        sig_w = wilcoxon[wilcoxon["significant"]] if "significant" in wilcoxon.columns else pd.DataFrame()
        lines.append(f"\n  Wilcoxon signed-rank pairwise comparisons:")
        lines.append(f"    Total pairs tested: {len(wilcoxon)}")
        lines.append(f"    Significant (p<0.05): {len(sig_w)}/{len(wilcoxon)}")
        for _, r in wilcoxon.iterrows():
            sig_mark = "*" if r.get("significant", False) else " "
            lines.append(
                f"    {r['var1']} vs {r['var2']}: "
                f"W={r.get('wilcoxon_stat', 0):.1f}, "
                f"p={r.get('wilcoxon_p', 0):.4f} {sig_mark}"
            )
    else:
        lines.append("\n  Wilcoxon pairwise tests not available.")
    lines.append("")


def section_top_companies(df, lines):
    lines.append(section_header("19. TOP RANKED COMPANIES"))
    if df is None or "pref_balanced" not in df.columns:
        lines.append("  Results not available.\n")
        return

    top = df.nlargest(20, "pref_balanced")
    cols = ["ticker", "company_name", "sector", "country", "pref_balanced",
            "ESG_composite", "financial_score"]
    avail = [c for c in cols if c in top.columns]

    lines.append(f"  Top 20 by Balanced Preference Score:")
    lines.append(f"  {'#':<4s} {'Ticker':<12s} {'Sector':<25s} {'Country':<8s} "
                 f"{'Pref':>6s} {'ESG':>6s} {'Fin':>6s}")
    lines.append("  " + "-" * 71)
    for i, (_, r) in enumerate(top.iterrows(), 1):
        sector = str(r.get("sector", ""))[:24]
        country = str(r.get("country", ""))[:7]
        lines.append(
            f"  {i:<4d} {r['ticker']:<12s} {sector:<25s} {country:<8s} "
            f"{r.get('pref_balanced', 0):6.1f} "
            f"{r.get('ESG_composite', 0):6.1f} "
            f"{r.get('financial_score', 0):6.1f}"
        )
    lines.append("")


def section_figures_list(lines):
    lines.append(section_header("20. FIGURES GENERATED"))
    figs = sorted(FIGURES.glob("*.png"))
    if not figs:
        lines.append("  No figures found.\n")
        return

    lines.append(f"  Total figures: {len(figs)}\n")
    for fig in figs:
        lines.append(f"  - {fig.name}")
    lines.append("")


def section_tables_list(lines):
    lines.append(section_header("21. TABLES GENERATED"))
    tables = sorted(TABLES.glob("*.csv"))
    if not tables:
        lines.append("  No tables found.\n")
        return

    lines.append(f"  Total tables: {len(tables)}\n")
    for t in tables:
        size_kb = t.stat().st_size / 1024
        lines.append(f"  - {t.name:<50s} ({size_kb:6.1f} KB)")
    lines.append("")


# ---------------------------------------------------------------------------
# Key Findings CSV
# ---------------------------------------------------------------------------

def generate_key_findings(df):
    findings = []

    # 1. Sample size
    if df is not None:
        findings.append({
            "category": "Data",
            "finding": f"Sample of {len(df)} companies across "
                       f"{df['sector'].nunique() if 'sector' in df.columns else '?'} sectors",
            "significance": "N/A",
        })
        if "country" in df.columns:
            for c, n in df["country"].value_counts().items():
                findings.append({
                    "category": "Data",
                    "finding": f"{c}: {n} companies",
                    "significance": "N/A",
                })

    # 2. ESG-financial link
    reg = safe_read("esg_financial_regression.csv")
    if reg is not None:
        for _, r in reg.iterrows():
            if r["p_value"] < 0.05:
                findings.append({
                    "category": "ESG-Financial",
                    "finding": f"{r['independent_var']} significantly predicts "
                               f"{r['dependent_var']} (R2={r['r_squared']:.4f})",
                    "significance": f"p={r['p_value']:.4f}",
                })

    # 3. Sector effects
    anova = safe_read("sector_anova.csv")
    if anova is not None:
        for _, r in anova[anova["significant_anova"]].iterrows():
            findings.append({
                "category": "Sector Differences",
                "finding": f"Significant sector differences in {r['variable']} "
                           f"(eta2={r['eta_squared']:.3f})",
                "significance": f"p={r['anova_p']:.4f}",
            })

    # 4. Country effects
    country = safe_read("country_differences.csv")
    if country is not None:
        for _, r in country[country["significant_t"]].iterrows():
            findings.append({
                "category": "Country Differences",
                "finding": f"Significant country difference in {r['variable']} "
                           f"(d={r['cohens_d']:.3f})",
                "significance": f"p={r['ttest_p']:.4f}",
            })

    # 5. Weight stability
    stab = safe_read("rank_stability.csv")
    if stab is not None:
        stable_pct = stab["rank_stable"].mean() * 100
        findings.append({
            "category": "Robustness",
            "finding": f"{stable_pct:.1f}% of companies have stable rankings "
                       f"under weight perturbation",
            "significance": "N/A",
        })

    # 6. VIF
    vif = safe_read("vif_multicollinearity.csv")
    if vif is not None:
        max_vif = vif["VIF"].max()
        findings.append({
            "category": "Multicollinearity",
            "finding": f"Max VIF = {max_vif:.2f} ({vif.loc[vif['VIF'].idxmax(), 'factor']})",
            "significance": "High" if max_vif > 10 else ("Moderate" if max_vif > 5 else "Low"),
        })

    return pd.DataFrame(findings)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 70)
    print("STEP 09: GENERATE RESEARCH SUMMARY REPORT")
    print("=" * 70)

    df = load_indexed()
    lines = []

    lines.append("=" * 72)
    lines.append("MULTI-FACTOR ESG-INTEGRATED INVESTMENT INDEX")
    lines.append("Research Summary Report")
    lines.append("=" * 72)
    lines.append("")
    lines.append("Generated from the complete analysis pipeline.")
    lines.append("All statistical tests use significance level alpha = 0.05.")
    lines.append("* indicates statistical significance at the 5% level.")
    lines.append("")

    section_overview(df, lines)
    section_normality(lines)
    section_correlations(lines)
    section_esg_financial(lines)
    section_sector_country(lines)
    section_quintile(lines)
    section_factor_contribution(lines)
    section_multicollinearity(lines)
    section_weight_sensitivity(lines)
    section_benchmark(lines)
    section_advanced(lines)
    section_multi_horizon(lines)
    section_alpha_beta(lines)
    section_regime(lines)
    section_monotonicity(lines)
    section_data_quality(lines)
    section_binary_ordinal(lines)
    section_nonparametric(lines)
    section_top_companies(df, lines)
    section_figures_list(lines)
    section_tables_list(lines)

    # Write report
    report_text = "\n".join(lines)
    REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
    REPORT_PATH.write_text(report_text, encoding="utf-8")
    print(f"[OK] Research summary: {REPORT_PATH} ({len(lines)} lines)")

    # Key findings CSV
    key_df = generate_key_findings(df)
    key_df.to_csv(KEY_FINDINGS_PATH, index=False, encoding="utf-8")
    print(f"[OK] Key findings: {KEY_FINDINGS_PATH} ({len(key_df)} findings)")

    print(f"\n[DONE] Report generation complete.")


if __name__ == "__main__":
    main()
